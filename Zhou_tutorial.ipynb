{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### introduction\n",
    "\n",
    "Here we will go through application of the Zhou partitioning algorithm to eddy covariance data. The script is designed to run on [FLUXNET2015](https://fluxnet.fluxdata.org/data/fluxnet2015-dataset/) .csv files directly, which ensures consistent variable names, processing and units. The tutorial will use data from the [Hyytiala forest](http://sites.fluxdata.org/FI-Hyy/) in Finland, but can be applied to any FLUXNET2015 dataset.\n",
    "\n",
    "Some experience in Python will make things easy, but I will try to explain the process step by step so as to be accessible to all backgrounds.\n",
    "\n",
    "### first things first\n",
    "\n",
    "The firs step is to import all needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr # labelled multi-dimensional arrays that are compatable with netcdf formats\n",
    "import numpy as np # numerical python for working with basic n-dimensional array\n",
    "import warnings # standard library for suppressing warnings\n",
    "\n",
    "# This secion facilitates easy plotting using xarray/pandas\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "# These two packages are imported from local scripts in the repository\n",
    "from preprocess import build_dataset # Converts the .csv to a labeled xarray Dataset\n",
    "import bigleaf # Used for converting units (e.g. LE to ET)\n",
    "import zhou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the dataset\n",
    "\n",
    "Here we used the build_datset function from the preprocess.py script, which chooses a subset of the variables from the FULLSET, as well as setting units, descriptive names, and site metadata into the file. Furthermore, only the years from 2008 to 2013 are seleced from the full timeseries. Comment out the second line if you would like to run the full datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec = build_dataset(\"FLX_FI-Hyy_FLUXNET2015_FULLSET_HH_2008-2010_1-3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check for an hourly file\n",
    "\n",
    "**build_dataset** also check to see if the file is at hourly or half hourly resolution. We will be working with ET in units of mm per timestep, and therefor some calculations are slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ec.agg_code == 'HH':\n",
    "    hourlyMask      = xr.DataArray(np.ones(ec.time.shape).astype(bool),coords=[ec.time],dims=['time'])\n",
    "    nStepsPerDay    = 48\n",
    "else:\n",
    "    hourlyMask      = np.ones(ec.time.shape).astype(bool)\n",
    "    hourlyMask[::2] = False\n",
    "    nStepsPerDay = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unit conversions\n",
    "\n",
    "Here we convert the estimated LE values to ET using the functions found in bigleaf.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec['ET'] = (bigleaf.LE_to_ET(ec.LE, ec.TA)*60*60*(24/nStepsPerDay))\n",
    "ec['ET'] = ec['ET'].assign_attrs(long_name = 'evapotranspiration',\n",
    "                           units     = 'mm per timestep')\n",
    "\n",
    "# fill missing NETRAD values with LE+H+G\n",
    "ec['NETRAD'][np.isnan(ec['NETRAD'])] = ec['LE'][np.isnan(ec['NETRAD'])]+ec['H'][np.isnan(ec['NETRAD'])]+ec['G'][np.isnan(ec['NETRAD'])]\n",
    "\n",
    "PET, _ = bigleaf.PET(ec.TA, ec.PA, ec.NETRAD,\n",
    "                           G=ec.G, S=None, alpha=1.26,\n",
    "                           missing_G_as_NA=False, missing_S_as_NA=False)\n",
    "ec['PET'] = PET*60*60*(24/nStepsPerDay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial checks\n",
    "\n",
    "Here we can do an initial precheck to see if the data looks reasonable. This one line will aggrigate the sub-daily flux to a daily sum. Note that if data is missing within a day, the entire day will return a NaN value (this can be changed by setting **skipna=True**). Feel free to test other variables by changing the variable name. Some good variables to check are GPP_NT and P. Daily means can also be calculated by changing **sum** to **mean**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec['PET'].resample(time='D').sum(skipna=False).plot(label='ET')\n",
    "ec['ET'].resample(time='D').sum(skipna=False).plot(label='PET')\n",
    "pl.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter data\n",
    "\n",
    "The first step in calculated transpiraiton from the Zhou method is to filter the data. Data filtration consists of four parts: quality, zero, season, and precip:\n",
    "\n",
    "| filter  | reason               | implementation                              |\n",
    "|:-------:|:--------------------:|:-------------------------------------------:|\n",
    "| quality | only good quality    | [NEE_QC, LE_QC, TA_QC, VPD_QC] == 0 or 1    |\n",
    "| zero    | only positive        | [ET, TA, VPD, NETRAD] > 0                   |\n",
    "| season  | only in season       | GPP_day > 10% of 95th percentile of GPP_day |\n",
    "| precip  | remove rainy periods | remove days with rain                       |\n",
    "|         |                      | remove 1 day after rain if P_day > PET_day  |\n",
    "|         |                      |remove 2 days after rain if P_day > 2*PET_day|\n",
    "\n",
    "From these four filters we can get the uWUEa_Mask and uWUEp_Mask as:\n",
    "\n",
    "uWUEa_Mask = zeroMask & qualityMask\n",
    "uWUEp_Mask = zeroMask & qualityMask & precipMask & seasonMask\n",
    "\n",
    "This has all been implement with the **zhouFlags** function in zhou.py, but it only works in an xarray dataset with the correct variable names. Note that the variant of GPP and be given (GPP_NT here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uWUEa_Mask, uWUEp_Mask = zhou.zhouFlags(ec, nStepsPerDay, hourlyMask,\n",
    "                                       GPPvariant='GPP_NT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting up the daily dataset\n",
    "\n",
    "The Zhou method estimates transpiration at the daily timescale, so we need to make a new dataset at a daily resolution. We will start with making a new dataset from ET, and then make two blank dataarrays for our Zhou method T estimates. This process is much easier when using Xarray's **resample** funciton. We will set **skipna=False** so if days have missing ET values, the result for that day will be NaN rather than skipping the missing value and underestimating ET for that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_zhou = ec[['ET']].resample(time='D').sum(skipna=False)\n",
    "ds_zhou['ET'] = ds_zhou['ET'].assign_attrs(\n",
    "            long_name = 'evapotranspiration',\n",
    "            units     = 'mm d-1')\n",
    "\n",
    "ds_zhou['zhou_T'] = ds_zhou['ET']*np.nan\n",
    "ds_zhou['zhou_T'] = ds_zhou['zhou_T'].assign_attrs(\n",
    "            long_name = 'Zhou estimated transpiration with uWUEa calculate for each day, using GPP_NT',\n",
    "            units     = 'mm d-1')\n",
    "\n",
    "ds_zhou['zhou_T_8day'] = ds_zhou['ET']*np.nan\n",
    "ds_zhou['zhou_T_8day'] = ds_zhou['zhou_T_8day'].assign_attrs(\n",
    "            long_name = 'Zhou estimated transpiration with uWUEa calculate for an 8 day moving window (centered), using GPP_NT',\n",
    "            units     = 'mm d-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### partitioning\n",
    "\n",
    "Now we can run the actual partitioning. Compared to the TEA and PÃ©rez-Priego methods, the Zhou method is very fast. Note that the GPP and VPD should be in gC m^-2 d^-1 and hPa, respectively. Also, the percentile used for calculating uWUEp can be specified as the rho parameter, here we use the 95th percentile which is recommending in Zhou et al 2016.\n",
    "\n",
    "The output will be the estimated uWUEp as a single value, and the timeseries of transpiration both with uWUEa calculated for each day individually (**zhou_T**) and using an 8 day moving window (**zhou_T_8day**). Each year will be calculated separately, giving an unique uWUEp for the year. Here we have a simple for loop for each year, then the partitioning is done on the half-hourly data. The daily dataset (**ds_zhou**) is then updated with the estimates for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET  = ec.ET.values\n",
    "GxV = (ec.GPP_NT*np.sqrt(ec.VPD)).values\n",
    "\n",
    "for year in np.unique(ec['time.year']):\n",
    "    yearMask = (ec['time.year']==year).values\n",
    "    uWUEp, zhou_T, zhou_T_8day = zhou.zhou_part(ET[yearMask], GxV[yearMask],\n",
    "                                          uWUEa_Mask[yearMask], uWUEp_Mask[yearMask],\n",
    "                                          nStepsPerDay, hourlyMask[yearMask],\n",
    "                                          rho = 95/100)\n",
    "    ds_zhou['zhou_T'][ds_zhou['time.year']==year] = zhou_T\n",
    "    ds_zhou['zhou_T_8day'][ds_zhou['time.year']==year] = zhou_T_8day\n",
    "    print(year,'uWUEp = ',uWUEp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial checks\n",
    "\n",
    "Now we can do some diagnostic plots for each estimate. First just plots of daily T for both the daily and 8 daily estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_zhou.ET.plot()\n",
    "ds_zhou.zhou_T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_zhou.ET.plot()\n",
    "ds_zhou.zhou_T_8day.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They both seem to agree. We can check directly by plotting them against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.scatter(ds_zhou.zhou_T,ds_zhou.zhou_T_8day)\n",
    "pl.plot([0,3], [0,3], '--k')\n",
    "pl.ylim(0,3)\n",
    "pl.xlim(0,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can look at the mean seasonal cycle of ET, zhou_T_8day, and zhou_E_8day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_zhou.ET.groupby('time.dayofyear').mean().plot()\n",
    "ds_zhou.zhou_T_8day.groupby('time.dayofyear').mean().plot()\n",
    "(ds_zhou.ET-ds_zhou.zhou_T_8day).groupby('time.dayofyear').mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data output\n",
    "\n",
    "If you want to save your partitioned data, we can export directly to a netcdf format, which will preserve the current shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_zhou.to_netcdf('FI-Hyy_Zhou_output.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or back to a .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_zhou.to_dataframe().to_csv('FI-Hyy_Zhou_output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
